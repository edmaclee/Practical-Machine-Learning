
## The data from this report comes from 
### http://groupware.les.inf.pucrio.br/har 
## The training data is available from
### https://d396qusza40orc.cloudfront.net/predmachlearn/pmltraining.csv
## The test data is available from
### https://d396qusza40orc.cloudfront.net/predmachlearn/pmltesting.csv

### To achieve a consistent result, 1234 is used as the pseudorandom seed. The caret and randomForest packages are needed to build the model.

### set env, read in data set and set pseudorandom seed to 1234
```{r}
setwd("C:/Users/momlsy/Documents/assignment")
trainset <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testset <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
set.seed(1234)
```

## Coherence test
```{r}
all.equal(colnames(testset)[1:length(colnames(testset))-1], colnames(trainset)[1:length(colnames(trainset))-1])
```

### Cleaning dataset 
```{r}
mytrainset <- trainset[,colSums(is.na(trainset)) == 0]
mytestset <- testset[,colSums(is.na(testset)) == 0]
```

### Filter irrelevant variables (columns) in data set
```{r}
trainset <- mytrainset[,-c(1:7)]
testset <- mytestset[,-c(1:7)]
```

## Partition training data set for cross validation
### Trainset data set is partitioned into 2 sets - training 60% & test 40%
```{r}
library(caret)
subsamples <- createDataPartition(y=trainset$classe, p=0.60, list=FALSE)
mysubtrain <- trainset[subsamples,]
mysubtest <- trainset[-subsamples,]
dim(mysubtrain)
dim(mysubtest)
```

### Plotting bar chart using various classe levels from partitioned training data set
```{r}
plot(mysubtrain$classe, col="cyan", main="Bar Chart for the classe levels from partitioned Training data set", xlab="classe levels", ylab="Frequency")
```

## Observation
### Base on the chart, level A has the most frequent occurrences while level D with the least occurrences.

### Model 1 - using Decision Tree
```{r results='hide'}
library(rpart)
```
```{r}
model1 <- rpart(classe ~ ., data=mysubtrain, method="class")
predict1 <- predict(model1, mysubtest, type = "class")
```

```{r results='hide'}
library(rpart.plot)
```
```{r}
rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
confusionMatrix(predict1, mysubtest$classe)
```

## Observation from model 1
### Report shows that the accuracy of model 1 using decision tree is 0.739

## Model 2 - using randomForest function
```{r results='hide'}
library(randomForest)
```
```{r}
model2 <- randomForest(classe ~. , data=mysubtrain)
predict2 <- predict(model2, mysubtest, type = "class")
```
## Testing result on testing data set
```{r}
confusionMatrix(predict2, mysubtest$classe)
```
## Conclusion from model 2
### Statistic shows the accuracy of model 2 using randomForest is 0.993. The out-of-sample error is only 1-0.993=0.007. This shows that randomForest algorithm perform better accuracy than Decision Tree

## Applying the algorithm to 20 test cases
```{r}
predict <- predict(model2, testset, type="class")
head(predict,20)
```

## Write files for submission
```{r}
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predict)
```


